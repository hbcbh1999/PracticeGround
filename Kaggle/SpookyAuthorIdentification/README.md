[https://www.kaggle.com/theimgclist/spooky-nlp-and-topic-modelling-tutorial]      
Found this dataset and problem in a stack overflow answer [here](https://datascience.stackexchange.com/a/27496/46989).  
I went through a kernel and two other notebooks of this problem.  
I learnt many new things through this problem.  
I now know how to make a WordCloud and it's very intersting.   
I came to know how NLTK's vectorizer and Sklearn's vectorizer are different.Quoting an explanation from the kernel:  
"Unlike the NLTK tokenizer that you were introduced to in the Section 2a earlier, Sklearn's tokenizer discards all single character terms like ('a', 'w' etc)  and also lower cases all terms by default.   
Filtering out stopwords in Sklearn is as convenient as passing the value 'english' into the argument "stop_words" where a built-in English stopword list is automatically used."  
A very useful kernel to know the steps involved in NLP.  
